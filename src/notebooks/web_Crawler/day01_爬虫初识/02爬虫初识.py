# 【一】爬虫简介
# 爬虫就是利用代码在互联网中自动获取信息的技术
# 主要是通过代码模拟浏览器的行为，向目标服务器发送请求 得到响应数据，解析响应数据 清洗数据 得到我们想要的数据

# 可以应用到 新闻、论坛、小说、视频 ...

# 【二】工作原理
# 通过代码模拟 浏览器 发起 HTTP请求 给服务器 得到响应数据，解析响应数据 清洗数据
# 清洗数据 (根据自己的规则进行过滤) 就是用 padans(pd 解析 excel 数据) polars(pl 解析 excel 数据) / matlib (画图的) / echarts (前端的可视化UI界面)

# 【三】爬虫的分类
# 【1】通用爬虫和定向爬虫
# 通用爬虫就是整个页面上的所有数据都抓取
# 定向爬虫就是对指定目标数据进行抓取

# 做京东商品数据的专区 通用爬虫 商品名称，价格，销量，简介，评论 ... (任何数据我都要.)
# 做京东商品数据的专区 定向爬虫 商品名称，价格，销量 (只选择自己需要的指定的数据.)

# 【2】基于规则的爬虫 和 基于机器学习的爬虫
# 基于规则 提前定义好那些数据需要 就抓取制定的数据
# 基于机器学习 给代码制定一个游戏规则 当他抓取到制定数据之后就给他奖励 自己的奖励值越多 他就越知道自己应该抓取那些数据

# 【3】单机爬虫和分布式爬虫
# 单机 在一台机器上去抓取数据
# 分布式 给定多台机器同时抓取数据 一台机器上开多个虚拟机

# 【四】爬虫经常用的库
# 编程语言：Python -- 有大佬帮助我们写好了很多第三方的库 ， Python语言比较简洁
# 【1】requests
# 帮助我们模拟浏览器发起HTTP请求的库
# 【2】BeautifulSoup
# 用来解析获取到的文档对象，通过对象的方法提取数据 ---- 复杂
# 【3】lxml
# 用来解析获取到的文档对象，通过对象的方法提取数据 --- 非常简单
# 【4】fake-useragent
# 模拟浏览器，给我们提供浏览器的标识凭证
# 【5】Scrapy
# 大佬们开发的一套完整的爬虫框架内置了数据解析数据清洗数据存储等等功能
# 九跟你的Django框架一样 大而全
# 【6】Selenium
# 有些情况下我们是没办法获取更多数据的
# 自动化控制浏览器获取响应数据 模拟人手的真实操作

# 【五】爬虫的注意事项
# 【1】爬虫要合法
# 别做违法犯罪的事，如果你以后真的感兴趣 去深学，一定要注意 黄赌毒 别干，恶意采集 ---> 会导致网站崩溃
# 【2】爬虫的速度要适中
# 爬的太快一是容易触发风控 二是容易对网站产生冲击
# 适当加 sleep --- 没事多加几个 sleep
# 【3】爬虫的稳定性
# 别遇到一个bug 整个就崩了
# 【4】爬虫数据的存储
# 适当的选择存储方式

# 【六】爬虫的应用领域
# 【1】数据采集与挖掘
# ● 爬虫可以帮助从互联网上获取大量的数据，如新闻文章、商品信息、社交媒体内容等。这些数据可以被用于市场调研、舆情监控、数据分析等用途。
# 【2】搜索引擎
# ● 爬虫是搜索引擎的基础，通过爬虫程序可以自动抓取互联网上的网页内容，为搜索引擎建立索引，以便用户能够快速准确地搜索到所需的信息。
# 【3】价格比较与竞品分析
# ● 通过爬虫可以定期获取竞争对手的产品信息和价格，以便进行价格比较和竞品分析，从而提供有效的市场竞争策略。
# 【4】舆情监控与声誉管理
# ● 通过爬虫可以实时监测和分析社交媒体、论坛、新闻等渠道中与企业或个人相关的言论和评论，以及舆情走势，帮助企业及时了解公众对其品牌声誉和产品的看法，进行声誉管理和危机处理。
# 【5】公共数据监测与分析
# ● 政府机构可以利用爬虫技术监测和分析公共数据，如气象数据、环境数据、人口统计数据等，以便为决策提供及时的数据支持。
# 【6】金融市场分析
# ● 通过爬虫可以获取金融市场中的股票数据、财务报表、新闻公告等信息，为投资者提供决策参考。
# 【7】学术研究与文献检索
# ● 研究人员可以利用爬虫收集相关领域的学术论文、研究成果等信息，帮助他们进行学术研究和文献综述。
# 【8】其他
# ● 爬虫在自动化测试之外还广泛应用于数据采集、舆情监控、价格比较、金融市场分析等多个领域。
# ● 通过爬虫技术，能够高效地获取和处理大规模的数据，为各行各业带来便利和价值。

# 【七】常见的反扒措施
# 【1】频率限制
# ● 网站会针对某个IP地址或用户账号设置请求频率限制，如单位时间内只允许发送一定数量的请求。
# ● 一旦超出限制，网站会对该IP或账号进行处罚，如暂时封禁或限制访问。
# 【2】封IP和封账号
# ● 网站可以通过监测异常行为，如频繁请求、高并发等来判断是否有恶意爬取行为，并对相关IP地址或账号进行封禁。
# ● 为了规避封禁，爬虫可以使用代理池来随机切换IP地址，或使用大量小号（账号池）轮流发送请求。
# 【3】请求头中带加密信息
# ● 网站可能要求请求头中包含特定的加密信息，如Referer（来源页面地址）和User-Agent（浏览器标识），用于验证请求的合法性。
# ● 爬虫需要模拟真实浏览器的请求头信息，以避免被检测为非法爬虫。
# 【4】响应回来的数据是加密
# ● 为了防止直接获取数据，网站可能会对返回的数据进行加密或编码，爬虫需要解密或解码才能获取到有效信息。
# ● 这种情况下，爬虫可能需要分析加密算法或从其他渠道获取解密密钥。
# 【5】验证码反扒
# ● 网站为了防止机器自动注册、恶意爬取等行为，可能会在关键操作前设置验证码。
# ● 爬虫需要通过第三方平台或自己破解验证码来进行自动化操作。
# 【6】JS加密
# ● 网站可能使用 JavaScript 对核心代码进行了压缩和混淆，以 ers() -> function() 的形式， ers.somethin() -> function somethin() ，并添加了一些晦涩的加密方法。
# ● 爬虫需要逆向工程来还原和理解这些加密算法，并编写相应的代码进行解密。
# 【7】手机设备唯一ID号
# ● 网站可能会根据爬虫请求的设备唯一标识符（例如IMEI、Android ID、iOS设备ID等）进行识别和限制。
# ● 爬虫可能需要模拟不同设备的请求，或者通过修改设备信息达到绕过检测的效果。
# 需要注意的是，对于某些网站，爬虫绕过这些反爬虫措施可能涉及到违法行为，建议在合法范围内开展爬虫活动，遵守相关法律法规和网站的使用规定。

# 如果你以后非常感趣
# 没事干爬虫的时候 多看看 ---****刑法****---
# 破坏计算机系统罪、贩卖敏感信息
